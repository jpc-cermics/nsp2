% -*- mode: latex -*-

\mansection{fsolve\_lsq}
\begin{mandesc}
  \shortunder{fsolve\_lsq}{fsolve_lsq}{non linear least square solver}
\end{mandesc}

% -- Calling sequence section
\begin{calling_sequence}
\begin{verbatim}
xopt = fsolve_lsq(x0, f, m)
[xopt, fopt ] = fsolve_lsq(x0, f, m, xtol=er, ftol=er, gtol=ea, args=obj, jac=Jf, maxfev=p, scale=vect, warn= bool) 
[xopt, fopt, info, nfev, njev] = fsolve_lsq(x0, f, m, xtol=er, ftol=er, gtol=ea, args=obj, jac=Jf, maxfev=p, scale=vect, warn= bool) 
[fx,Jfx]= fsolve_lsq_jac(x, f, m, args=obj);
\end{verbatim}
\end{calling_sequence}
% -- Parameters
\begin{parameters}
  \begin{varlist}
    \vname{x0}: real vector (says of length $n$), initial estimate of the optimal vector
    \vname{f}: nsp function (or string for an C or fortran external)
    \vname{m}: integer, number of equations
    \vname{xtol=er, ftol=er, gtol=ea}: optional named arguments relative to the stopping test (default:
    \verb+xtol=ftol=sqrt(2*%eps)+, \verb+gtol=0+).
    \vname{jac=Jf}: optional named argument. Jf should be a nsp function (the Jacobian of \verb+f+)
    \vname{args=obj}: optional named argument (could be any nsp object), useful to pass additional parameters for \verb+f+ (and
    Jf if provided).
    \vname{maxfev=p}: optional named argument, maximum number of function evaluations (default is $100(n+1)$ if Jf is
    provided and $200(n+1)$ otherwise).
    \vname{scale}: vector of length $n$, scaling factors.
    \vname{warn}: boolean. When the function is called with \verb+lhs <= 2+, then a message describing the 
    stopping condition is displayed except if \verb+warn=%f+.

    \vname{xopt}: vector of length $n$, should be the solution of the optimisation problem 
    \vname{fopt}: vector of length $n$, the value of \verb+f+  at \verb+xopt+
    \vname{info}: integer scalar, give information about the stopping test.
    \vname{nfev, njev}: integer scalars, respectively the number of calls to \verb+f+and Jf (njev=0 when jac is not provided)
  \end{varlist}
\end{parameters}

\begin{mandescription}
\verb+fsolve_lsq+  tries to find a minimum $x_{opt}$ of the sum of the square of the $m$ components of a function
of $n$ variables $f: {\R}^n \rightarrow  {\R}^m$ ($n$ is the number of unknowns and $m$ the number of equations
and we must have $m \ge n$):
$$
    {\cal F}(x) \equiv \frac{1}{2} \sum_{j=1}^m \left(f_j(x)\right)^2
$$
by an iterative algorithm (the Levenberg-Marquard method), starting from an initial guess \verb+x0+.
\verb+fsolve_lsq+ uses the routines lmder, lmdif from Minpack. In the following we will note $J_f(x)$
the Jacobian of $f$ at $x$, which is a $m \times n$ matrix, its element at the i th row and j th column
is:
$$
[J_f(x)]_{i,j} = \partial_{x_j} f_i(x)
$$
Also $[J_f(x)]_i$ will denote the i th row of the Jacobian and  $[J_f(x)]^j$ will denote its j th column. 

\itemdesc{application}
\verb+fsolve_lsq+ is useful for fitting a model to datas. In this case you have a ``model'' 
says, $y = {\cal M}(t, x)$, where:
\begin{itemize}
\item $t$ is the vector of the independant variables (or explanatory, or regressor variables);
\item $y$ is the vector of the response variables; 
\item and $x$ is the vector of the unknown $n$ parameters.
\end{itemize}
and you have a collection of $m$ observation datas $(t_i, y_i)$ and want to find the parameters $x$ such 
that the model fits them. A usual way to do this is to minimize the sum of squares of the errors:
$$
   \min_x \sum_{j=1}^m || \omega_j ({\cal M}(t_j, x) - y_j) ||^2
$$
here $\omega_j$ is a weight (or a diagonal matrix of weight) to take into account the fact that some observations
of $y_j$ are less or more accurate than others. Assuming that the response variable $y$ is a scalar, the function $f$
to build for \verb+fsolve_lsq+ is:
$$
  f_j(x) =  \omega_j ({\cal M}(t_j, x) - y_j) 
$$ 
and the $j$ th row of the Jacobian of $f$ is:
$$
  [J_f(x)]_j = \omega_j \nabla_x {\cal M}(t_j, x) =  \omega_j \left[ \partial_{x_1}{\cal M}(t_j, x), \dots,
    \partial_{x_n}{\cal M}(t_j, x) \right]
$$
So in general you will have to pass additionnal parameters (other than $x$) to the 
function \verb+f+ (and to its jacobian if you provide it) like the vectors with the 
weights $\omega_j$ and the $t_j$ and $y_j$ values of this example. This is the 
role of the \verb+args+ parameter. 
%Note that when {\cal M} is linear in $x$, that is:
%$$
%{\cal M}(t,x) = \sum_{i=1}^m \phi_i(t) x_i
%$$
%{\cal F} is reduced to a quadratic function and the problem could be solved by linear algebra only.


\itemdesc{scaling factors}
Scale factors are positive numbers such that the scaled variables $s_i x_i$ are of same order
(says they are roughly between $-1$ and $1$. If you know the approximate magnitude of the unknown
parameters, for instance you know that the first parameter $x_1$ is between $-10^{-7}$ and $10^{-6}$, 
the second is between $-50$ and $80$ the third is around  $10^{5}$ then you can choose $s_1 = 10^6$, 
$s_2 = 0.02$ and $s_3 = 10^{-5}$. When the scale factors are not provided by the user, the code 
computes them automatically (using the euclidian norm of the columns of the Jacobian). Scaling is 
important when the magnitude of the variables are really different (as in the example before). In 
this case if you known (even very approximatively) the scale of each parameter it is better
to provide them.  

\itemdesc{stopping tests}
Iterations are stopped when:
\begin{enumerate}
\item the absolute value of each component of the scaled gradient of ${\cal F}$ is under $gtol$: 
$$
||\nabla{\cal F}(x_{scaled})||_{\infty} = \max_i \frac{1}{s_i} | \partial_{x_i} {\cal F}(x) | =
\max_i \frac{1}{s_i} | [J_f(x)]^i \cdot f(x) | \le gtol
$$
In this case the \verb+info+ variable is set to 4. Remark that the i th component of the (non scaled) 
gradient is equal to the dot product between the i th column of the Jacobian and the function.
So this stopping test corresponds to the near orthogonality of $f$ with the columns of its
Jacobian.
\item if the first condition is not met then the code tests:
      \begin{enumerate}
      \item if the actual and the predicted relative reduction of {\cal F} is under $ftol$, in which case \verb+info+ is set to 1.
      \item if the relative displacement for $x$ is under $xtol$, in which case \verb+info+ is set to 2. 
      \end{enumerate}
      if both tests are satisfied then \verb+info+ is set to 3.
\item if any of these 3 tests is not satisfied, the code tests if the number of calls to the function has reached or 
      exceeded \verb+maxfev+, in this case , \verb+info+ is set to $5$.
\item if \verb+maxfev+ is not reached, the code redo the 3 tests replacing $gtol$, $ftol$ and $xtol$ by the epsilon
      machine, we get:
      \begin{itemize}
      \item info=8 if the gradient test is satisfied,  
      \item info=7 if the x convergence test test is satisfied,  
      \item info=6 if the f convergence test test is satisfied.  
      \end{itemize}
\end{enumerate}
Note that, when \verb+fsolve_lsq+ is called with only 1 or 2 outputs arguments, an error is set 
when \verb+info <= 0+, and a warning is displayed in the nsp console when \verb+info >= 1+ except 
if \verb+warn+ is set to \verb+%f+. 

\itemdesc{form of the function {\tt f} and optional argument {\tt args}}
The function \verb+f+ takes one argument, a vector $x$ of length $n$ and possibly additional parameters which will be passed using the \verb+args=obj+ named optional argument. If \verb+f+ has the form:
\begin{quote}
{\tt function y = f(x,p1,...,pn) \\
      ....}
\end{quote}
then \verb+args={p1,...,pn}+. Note that if $f$ has only one additional parameter 
it is not mandatory to encapsulate it in a cell array (except when the parameter is it 
self a cell array !). The additional parameters could be any nsp object. 
The function
should return a vector of length $m$. It is possible to use a C or fortran function in place
of an nsp function, this will be described in a next version of this manual page.

\itemdesc{optional {\tt jac} parameter}
The solver needs $Jf$ the jacobian of $x \mapsto f(x)$ and this function can be given using 
\verb+jac=Jf+. When jac is not provided the solver computes an approximation using finite differences. 
The header of the nsp function which defines the Jacobian of \verb+f+ should be of the form:
\begin{quote}
{\tt function J = Jf(x) \\
      ....}\\
or:\\
{\tt function J = Jf(x,p1,...,pn) \\
     ....}
\end{quote}
the additionnal parameters (should be the same than for $f$) being provided using {\tt args=\{p1,p2,...,pn\}}. 
Note that when the Jacobian is not given it is approximated by finite differences and it is 
possible to get the result of the evaluated Jacobian using the function \verb+J=fsolve_lsq_jac(x,f,m)+.
This gives a feature to test your own Jacobian if provided by comparing its evaluation with the 
\verb+fsolve_lsq_jac+ evaluation. Note that the same possibility is also available using the 
\manlink{derivative}{derivative} function. 

\end{mandescription} 

\begin{examples}
\paragraph{example 1} 
We consider a simple exponential decay model:
$$
 y = A \exp(-\frac{t}{\tau})
$$
and we want to identify the 2 parameters $A$ and $\tau$ (\verb+fsolve_lsq+ is not
mandatory for such a model because it can be linearized by taking the $\log$ but
it is a simple example to begin with). 

\begin{mintednsp}{nsp}
xclear();
function y = expdecay(t, A, tau)
   y = A*exp(-t/tau) 
endfunction

// build the function f for fsolve_lsq
function e = fun(x, w, t, y)
   A = x(1); tau = x(2);
   e = w.*( expdecay(t, A, tau) - y )
endfunction

// datas observations
m=10;    // nb of measures...
tm=[0.25; 0.5; 0.75; 1.0; 1.25; 1.5; 1.75; 2.0; 2.25; 2.5];
ym=[0.79; 0.59; 0.47; 0.36; 0.29; 0.23; 0.17; 0.15; 0.12; 0.08];
wm=ones(m,1); // measure weights (here all equal to 1)

// initial parameters guess
x0 = [1.5 ; 0.8];
// call to fsolve_lsq
xopt = fsolve_lsq(x0,fun, m, args={wm,tm,ym})

// display results
tt = linspace(0,1.1*max(tm),100)';
yy = expdecay(tt, xopt(1),xopt(2));
xbasc()
plot2d(tm, ym, style=-2,leg="measure points")
plot2d(tt, yy, style = 2,leg="fitted curve", leg_pos="dl")
xtitle("a simple fit with fsolve_lsq")

// same example providing the Jacobian
// the gradient of the model versus its parameters A and tau
function dy = Dexpdecay(t, A, tau)
   dy = [exp(-t/tau), (A/tau^2)*(t.*exp(-t/tau))]
endfunction

// build the Jacobian of f
function J = Jfun(x, w, t, y)
   A = x(1); tau = x(2);
   J = Dexpdecay(t, A, tau)
   J.scale_rows[w]
endfunction

// verifying the Jacobian with fsolve_lsq_jac
x = [0.5; 2]; 
[fx, Jfx] = fsolve_lsq_jac(x, fun, m, args={wm,tm,ym});
Jfxe =  Jfun(x,wm,tm,ym);
norm(Jfx-Jfxe,1)/norm(Jfxe)

// verifying the Jacobian with derivative
Jfx = derivative(fun,x, args={wm,tm,ym});
Jfxe =  Jfun(x, wm, tm, ym);
norm(Jfx-Jfxe,1)/norm(Jfxe)

// call fsolve using jac=
xopt = fsolve_lsq(x0,fun, m, jac=Jfun, args={wm,tm,ym})
\end{mintednsp}

\end{examples}

\begin{manseealso}
  \manlink{fsolve}{fsolve}, \manlink{derivative}{derivative}
\end{manseealso}

% -- Authors
\begin{authors}
 lmder, lmdif: Jorge More, Burt Garbow, and Ken Hillstrom. Nsp interface: Jean-Philippe Chancelier, Bruno Pincon
\end{authors}
