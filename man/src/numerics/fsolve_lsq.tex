% -*- mode: latex -*-

\mansection{fsolve\_lsq}
\begin{mandesc}
  \shortunder{fsolve\_lsq}{fsolve_lsq}{non linear least square solver}
\end{mandesc}

% -- Calling sequence section
\begin{calling_sequence}
\begin{verbatim}
xopt = fsolve_lsq(x0, f, m)
[xopt, fopt ] = fsolve_lsq(x0, f, m, xtol=er, ftol=er, gtol=ea, args=obj, jac=Df, maxfev=n, scale=vect, warn= bool) 
\end{verbatim}
\end{calling_sequence}
% -- Parameters
\begin{parameters}
  \begin{varlist}
    \vname{x0}: real vector, initial estimate of the optimal vector
    \vname{f}: nsp function (or string for an C or fortran external)
    \vname{m}: integer, number of equations
    \vname{xtol=er, ftol=er, gtol=ea}: optional named arguments relative to the stopping test.
    \vname{jac=Df}: optional named argument. Df should be a nsp function (the Jacobian of \verb+f+)
    \vname{args=obj}: optional named argument (could be any nsp object), useful to pass additional parameters for \verb+f+ (and
    Df if provided).
    \vname{maxfev=n}: optional named argument, maximum number of function evaluations (default is $100(n+1)$)
    \vname{scale}: vector of length $n$, scaling factors.
    \vname{xopt}: vector of length $n$, should be the solution of the optimisation problem 
    \vname{fopt}: vector of length $n$, the value of \verb+f+  at \verb+xopt+
    \vname{info}: integer scalar, give information about the stopping test.
    \vname{warn}: boolean. When the function is called with \verb+lhs <= 2+, then a message describing the 
    stopping condition is displayed except if \verb+warn=%f+.
    \vname{nfev, njev}: integer scalar, respectively the number of calls to \verb+f+and Df (njev=0 when jac is not provided)
  \end{varlist}
\end{parameters}

\begin{mandescription}
\verb+fsolve_lsq+  tries to find a minimum $x_{opt}$ of the sum of the square of the $m$ components of a function
of $n$ variables $f: {\R}^n \rightarrow  {\R}^m$ ($n$ is the number of unknowns and $m$ the number of equations
and we must have $m \ge n$):
$$
    {\cal F}(x) \equiv \frac{1}{2} \sum_{j=1}^m \left(f_j(x)\right)^2
$$
by an iterative algorithm (the Levenberg-Marquard method), starting from an initial guess \verb+x0+.
\verb+fsolve_lsq+ uses the routines lmder, lmdif from Minpack. In the following we will note $J_f(x)$
the Jacobian of $f$ at $x$, which is a $m \times n$ matrix, its element at the i th row and j th column
is:
$$
[J_f(x)]_{i,j} = \partial_{x_j} f_i(x)
$$
Also $[J_f(x)]_i$ will denote the i th row of the Jacobian and  $[J_f(x)]^j$ will denote its j th column. 

\itemdesc{application}
\verb+fsolve_lsq+ is useful for fitting a model to datas. In this case you have a ``model'' 
says, $y = {\cal M}(t, x)$, where:
\begin{itemize}
\item $t$ is the vector of the independant variables (or explanatory, or regressor variables);
\item $y$ is the vector of the response variables; 
\item and $x$ is the vector of the unknown $n$ parameters.
\end{itemize}
and you have a collection of $m$ observation datas $(t_i, y_i)$ and want to find the parameters $x$ such 
that the model fits them. A usual way to do this is to minimize the sum of squares of the errors:
$$
   \min_x \sum_{j=1}^m || \omega_j ({\cal M}(t_j, x) - y_j) ||^2
$$
here $\omega_j$ is a weight (or a diagonal matrix of weight) to take into account the fact that some observations
of $y_j$ are less or more accurate than others. Assuming that the response variable $y$ is a scalar, the function $f$
to build for \verb+fsolve_lsq+ is:
$$
  f_j(x) =  \omega_j ({\cal M}(t_j, x) - y_j) 
$$ 
and the $j$ th row of the Jacobian of $f$ is:
$$
  [J_f(x)]_j = \omega_j \nabla_x {\cal M}(t_j, x) =  \omega_j \left[ \partial_{x_1}{\cal M}(t_j, x), \dots,
    \partial_{x_n}{\cal M}(t_j, x) \right]
$$
%Note that when {\cal M} is linear in $x$, that is:
%$$
%{\cal M}(t,x) = \sum_{i=1}^m \phi_i(t) x_i
%$$
%{\cal F} is reduced to a quadratic function and the problem could be solved by linear algebra only.


\itemdesc{scaling factors}
Scale factors are positive numbers such that the scaled variables $s_i x_i$ are of same order
(says they are roughly between $-1$ and $1$. If you know the approximate magnitude of the unknown
parameters, for instance you know that the first parameter $x_1$ is between $-10^{-7}$ and $10^{-6}$, 
the second is between $-50$ and $80$ the third is around  $10^{5}$ then you can choose $s_1 = 10^6$, 
$s_2 = 0.02$ and $s_3 = 10^{-5}$. When the scale factors are not provided by the user, the code 
computes them automatically (using the euclidian norm of the columns of the Jacobian). Scaling is 
important when the magnitude of the variables are really different (as in the example before). In 
this case if you known (even very approximatively) the scale of each parameter it is better
to provide them.  

\itemdesc{stopping tests}
Iterations are stopped when:
\begin{enumerate}
\item the absolute value of each component of the scaled gradient of ${\cal F}$ is under $gtol$: 
$$
||\nabla{\cal F}(x_{scaled})||_{\infty} = \max_i \frac{1}{s_i} | \partial_{x_i} {\cal F}(x) | =
\max_i \frac{1}{s_i} | [J_f(x)]^i \cdot f(x) | \le gtol
$$
In this case the \verb+info+ variable is set to 4. Remark that the i th component of the (non scaled) 
gradient is equal to the dot product between the i th column of the Jacobien and the function.
So this stopping test corresponds to the near orthogonality of $f$ with the columns of its
Jacobian.
\item if the first condition is not met then the code tests:
      \begin{enumerate}
      \item if the actual and the predicted relative reduction of {\cal F} is under $ftol$, in which case \verb+info+ is set to 1.
      \item if the relative displacement for $x$ is under $xtol$, in which case \verb+info+ is set to 2. 
      \end{enumerate}
      if both tests are satisfied then \verb+info+ is set to 3.
\item if any of these 3 tests is not satisfied, the code tests if the number of calls to the function has reached or 
      exceeded \verb+maxfev+, in this case , \verb+info+ is set to $5$.
\item if \verb+maxfev+ is not reached, the code redo the 3 tests replacing $gtol$, $ftol$ and $xtol$ by the epsilon
      machine, we get:
      \begin{itemize}
      \item info=8 if the gradient test is satisfied,  
      \item info=7 if the x convergence test test is satisfied,  
      \item info=6 if the f convergence test test is satisfied.  
      \end{itemize}
\end{enumerate}
Note that, when \verb+fsolve_lsq+ is called with only 1 or 2 outputs arguments, an error is set 
when \verb+info <= 0+, and a warning is displayed in the nsp console when \verb+info >= 2+ except 
if \verb+warn+ is set to \verb+%f+. 

\itemdesc{form of the function {\tt f}}
The function \verb+f+ takes one argument, a vector $x$ of length $n$ and possibly an additional
argument which will be passed using the \verb+args=obj+ named optional argument. The function
should return a vector of length $m$. It is possible to use a C or fortran function in place
of an nsp function, this will be described in a next version of this manual page.

\itemdesc{optional argument {\tt args}}
Additional parameters to the function \verb+f+ (and also to the function which computes the Jacobian 
if it is provided by the user) can be transmited using \verb+args=obj+. Where obj can be any nsp 
object (see examples). In this case \verb+f+ has the form:
\begin{quote}
{\tt function y = f(x,obj) \\
      ....}
\end{quote}

\itemdesc{optional {\tt jac} parameter}
The solver needs $Df$ the jacobian of $x \mapsto f(x)$ and this function can be given using 
\verb+jac=Df+. When jac is not provided the solver computes an approximation using finite differences. 
The header of the nsp function which defines the Jacobian of \verb+f+ should be of the form:
\begin{quote}
{\tt function J = Df(x) \\
      ....}\\
or:\\
{\tt function J = Df(x,obj) \\
     ....}
\end{quote}
when {\tt obj} is provided with {\tt args=obj}. 
Note that when the Jacobian is not given it is approximated by finite differences and it is 
possible to get the result of the evaluated Jacobian using the function \verb+J=fsolve_lsq_jac(x,f)+.
This gives a feature to test your own Jacobian if provided by comparing its evaluation with the 
\verb+fsolve_lsq_jac+ evaluation. Note that the same possibility is also available using the 
\manlink{derivative}{derivative} function. 

\end{mandescription} 

\begin{examples}
\paragraph{example 1} 
We consider a simple exponential decay model:
$$
 y = A \exp(-\frac{t}{\tau})
$$
and we want to identify the 2 parameters $A$ and $\tau$ (\verb+fsolve_lsq+ is not
mandatory for such a model because it can be linearized by taking the $\log$ but
it is a simple example to begin with). 

\begin{program}\HCode{function y = expdecay(t, A, tau)\Hnewline
   y = A*exp(-t/tau) \Hnewline
endfunction\Hnewline
\Hnewline
// build the function f for fsolve_lsq\Hnewline
function e = fun(x, obs)\Hnewline
   // obs is a hash table with the observations\Hnewline
   A = x(1); tau = x(2);\Hnewline
   e = obs.w.*( expdecay(obs.t, A, tau) - obs.y )\Hnewline
endfunction\Hnewline
\Hnewline
// datas observations\Hnewline
m=10;    // nb of measures...\Hnewline
tm=[0.25; 0.5; 0.75; 1.0; 1.25; 1.5; 1.75; 2.0; 2.25; 2.5];\Hnewline
ym=[0.79; 0.59; 0.47; 0.36; 0.29; 0.23; 0.17; 0.15; 0.12; 0.08];\Hnewline
wm=ones(m,1); // measure weights (here all equal to 1)\Hnewline
\Hnewline
// initial parameters guess\Hnewline
x0 = [1.5 ; 0.8];\Hnewline
// call to fsolve_lsq\Hnewline
xopt = fsolve_lsq(x0,fun, m, args=hash(3,t=tm,y=ym,w=wm))\Hnewline
\Hnewline
// display results\Hnewline
tt = linspace(0,1.1*max(tm),100)';\Hnewline
yy = expdecay(tt, xopt(1),xopt(2));\Hnewline
xbasc()\Hnewline
plot2d(tm, ym, style=-2,leg="measure points")\Hnewline
plot2d(tt, yy, style = 2,leg="fitted curve", leg_pos="dl")\Hnewline
xtitle("a simple fit with fsolve_lsq")}
\end{program}

\end{examples}

\begin{manseealso}
  \manlink{fsolve}{fsolve}, \manlink{derivative}{derivative}
\end{manseealso}

% -- Authors
\begin{authors}
 lmder, lmdif: Jorge More, Burt Garbow, and Ken Hillstrom. Nsp interface: Jean-Philippe Chancelier, Bruno Pincon
\end{authors}
